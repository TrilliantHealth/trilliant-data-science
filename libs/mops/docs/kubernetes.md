# Kubernetes

We use Kubernetes for scalable general-purpose compute as well as GPU-based compute. An example of GPU
usage can be found in [mldemo](../../../apps/mldemo/README.md).

This library allows imperative and declarative launching of K8s Jobs on specified Docker images, and will
by default scrape the logs of the pods run by the Jobs.

This is _not_ intended to replace the use of Helm for long-running services (e.g. APIs).

## Required Kubernetes Setup

This is now documented in the
[Guidebook](https://guidebook.trillianthealth.com/data-science/kubernetes-access/).

### Installation

You must add mops to your project with the `k8s` extras, or your imports of the `k8s` module will fail.
Use one of the following, depending on whether you're in the monorepo or not:

- `poetry add "../../libs/mops[k8s]" --editable`
- `pipenv install "thds-mops[k8s]"`

### Register your namespace for Workload Identity \[optional but recommended as of Q2 2023\]

It's recommended that you make a small PR\[^1\] to `engineering-infra`
[here](https://github.com/TrilliantHealth/engineering-infra/blob/main/engineering-stable/datascience/identities.tf#L4),
and a corresponding PR [in this library](../src/thds/mops/east_config.toml) to add the K8s namespace(s)
you're running under\[^2\]. This will automatically enable the use of
[more reliable and performant auth](https://azure.github.io/azure-workload-identity/docs/introduction.html)\[^3\]
from your K8s pods against Azure resources such as ADLS.

These steps are not required, and will also not benefit anyone who is not using the `mops` library as
part of their application (e.g. who is just deploying applications via Helm).

\[^1\]: Unfortunately this manual step appears to be unavoidable at this time, because the configuration
must live in a part of Azure that we do not have permission to directly edit.

\[^2\]: A K8s namespace using a sanitized version of your local computer's user account name will be
created if you use this library from your local machine without further configuration.

\[^3\]: If the namespace you're using does not appear in the list of
`common_azure_access_identity.namespaces` in `engineering-infra` and the corresponding default config
value for `namespaces_supporting_workload_identity` in `mops`, then you are more likely to run into
non-deterministic Azure authentication failures because of the
[older](https://learn.microsoft.com/en-us/azure/aks/use-azure-ad-pod-identity) authn/authz method that
must be used to support your unknown namespace.

## Declarative usage with mops `use_runner`:

This approach is preferred for cases where the code to be run on Kubernetes can be considered a
[pure function](./pure_functions.md) and is implemented as or wrapped by Python.

> If you're trying to wrap a Docker image that you can't or don't want to install `mops` on, you'll need
> to use the low-level imperative API instead.

You will need to construct a `k8s_shell` with configuration appropriate to the Python function that will
ultimately be invoked remotely. This includes specifying the name of your Docker container as well as
providing resource requests or limitations. For example, you might create a module called `k8s_shell.py`:

```python
import thds.mops.k8s as mops_k8s

df_k8s_spot_11g_shell = mops_k8s.k8s_shell(
    mops_k8s.autocr('ds/demand-forecast:latest'),
    # gotta specify which container this runs on
    node_narrowing=dict(
        resource_requests={"cpu": "3.6", "memory": "11G"},
        tolerations=[mops_k8s.tolerates_spot()],
    ),
)
```

> `mops` does _not_ distribute your code for you, and neither does the provided k8s shell. You must
> perform your own code distribution by building and pushing a Docker image to an accessible container
> registry, and configuring the K8s Shell so that it knows what pre-published image to use.

This will require your Docker image to exist and to be accessible to Kubernetes via the configured
Container Registry. Additionally, that image must somehow be provided with read and write access to ADLS,
since all data will be transferred back and forth from your local environment via ADLS. All Data Science
K8s clusters will provide read/write access to a set of ADLS Storage Accounts by default.

Once you have constructed an appropriate Shell, you need only pass it to the `MemoizingPicklingRunner`
constructor, and then use `use_runner` plus that runner to construct a decorator that will transmit
computation for any pure function to the remote K8s environment, and inject its result right back into
the local/orchestrator context.

## Imperative usage

Declarative usage is recommended because it confers additional capabilities such as memoization (and
therefore a form of fault-tolerance) but if you have a non-Python image you want to launch, see the docs
on the low-level imperative interface [here](../src/thds/mops/k8s/README.md).

The basic container specification will be the same regardless of which approach is used.

## Logging

For Kubernetes only, `mops` will automatically try to scrape and print the logs from each launched pod.
ANSI terminal Colors will be automatically chosen for each separate pod being scraped.

If you are running many pods in parallel, it may be desirable to turn off logging, as it has been
observed to leak memory over the course of long runs, and a separate OS thread is used for each pod being
scraped.

Either set the `MOPS_NO_K8S_LOGS` environment variable, or programmatically pass `suppress_logs=True` to
either `k8s_shell` (declarative API) or `launch` (imperative API).

## Common failure modes

> This list is not even close to exhaustive, but we can add to it over time.

### Too little memory.

Kubernetes will often
[kill pods that use more than their requested memory](https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits),
but what makes this sometimes tricky to diagnose is that Kubernetes will _not_ kill pods _because_ they
used more memory than requested - pod death will only occur when _other_ pods scheduled on the same node
also end up using more memory than they requested... and then the node itself runs out of RAM, and K8s
has to choose a pod to kill.

If you are CPU bound, you may as well request roughly 4x more RAM than CPU, since CPU-optimized nodes
generally have 4GB of RAM for each CPU. If you are strictly memory bound, you will simply need to request
enough memory for the worst case scenario for each of your Jobs.

If you have a hybrid workload (different types of Jobs running concurrently) and are willing to
occasionally let partially-complete pods die from OOM, you can set your memory requests lower than the
max pressure â€“ may the odds be ever in your favor ;)

### Missing Docker image.

We're working to make this easier to identify when using `mops`, but on a basic level, the Kubernetes API
does not ever reject a request to launch a Job or Pod when provided with a Docker image tag that it can't
find. It will happily accept your request, and then there will just be infinite ImageBackOff errors that
can be found by querying the API (or using a console like k9s). This is fundamentally because K8s wants
to make allowance for the possibility that your container registry might go down for periods of time,
which shouldn't prevent you from making a request for something to work once the registry comes back up.

In any case, if you're running a new process, or have recently changed something about your Docker image,
keep an eye on the API or console for ImageBackOff errors that might be an indication that you've asked
`mops` to create a Job with an image that simply does not exist.

Alternatively, you can take a look at the [machinery](../src/thds/mops/k8s/image_backoff) we have
[put in place](https://github.com/TrilliantHealth/demand-forecast/blob/main/demandforecast/k8s_choose_image.py#L18)
that may help you detect this.

## Orchestrator pods

If you're running a big or long-running process with [remote workers](./remote.md) in K8s, because of
common network [limitations](./limitations.md), you may wish to run from an orchestrator pod also hosted
in Kubernetes.

See [here](../src/thds/mops/k8s/orchestrator/README.md).

## Further reading

See [the source README](../src/thds/mops/k8s/README.md) for more specifics on things you can do with
Kubernetes via `mops`.
