link:../README.adoc[â†‘]

# Optimizations

Because transferring execution to a link:./remote.adoc[remote] runtime involves
link:./serialization.adoc[serialization], there can be a performance penalty to using `mops`, compared to running everything locally in shared memory.

Therefore, `mops` has added some optimizations to help avoid compute resource bottlenecks, particularly
on the link:./orchestrator.adoc[orchestrator] process.

## File transfer

****
ðŸ“¢ The TL;DR on File Transfer is that you should:

* replace function parameters representing readable/input data (e.g. `Path`, `AdlsFqn`, etc.) with
  `core.Source`, or `os.PathLike` if you want to be maximally open to callers.
* replace function return values with `core.Source`, e.g. `core.Source.from_file(aPath)`,
  `core.Source.from_uri(str(an_AdlsFqn))`.

These changes will, in general, incur minor changes for the calling code.
****

There are lots of Python objects that take up significantly more space in memory than on disk. A
parquet-ified DataFrame is a pretty common example. Your existing parallel code may even pass local paths
to separate processes so that you can avoid transferring large amounts of memory when it can more easily
be read from disk. This makes your functions _technically_ impure, but can certainly save time and occasionally
resources.

We provide several different optimizations for files, with different use cases. All will dramatically
reduce peak memory pressure on the orchestrator process, because the data gets streamed from/to disk
rather than being held in memory all at once. If you have concerns about memory pressure during the
'reduce' step of your application (i.e. as you are collecting large quantities of results from many
remote tasks), you should consider using one of these approaches rather than transferring in-memory bytes
objects.

- _A cloud-based blob store (e.g. ADLS) required for remote use_ - `mops` needs somewhere to put these files.

- _Local computation supported_ - However, all of these abstractions are also _usable_ with `mops` removed.
  This means that your pipeline can be expressed in terms of these file
  abstractions without actually requiring ADLS or `MemoizingPicklingRunner` or K8s, etc.

### thds.core.source

`thds.core.source` provides an abstraction targeted at read-only data. It allows `mops` to
provide significant optimizations over other options listed below.

NOTE: `core.Source` very carefully preserves the 'droppability' of `mops`. If you use it
to represent your inputs and outputs, you will not be stuck using `mops` and can easily
turn `mops` 'off' at any time while still getting efficient and sane file behavior on your
local machine.

On a basic level, `Source` represents two conceptually different possibilities for the availability of
your data. Data is either:

- available on the local filesystem
- available remotely, but able to be downloaded to the local filesystem when the data is opened for
  reading.footnote:[The current implementation does not provide for efficiently 'seeking' to a byte range within the
    Source - the entire file must first be downloaded. This is not a fundamental limitation, and could in
    theory be lifted by further technical work, but not all remote file stores would necessarily support
    this type of access anyway, and we have not (yet) found ourselves in need of the capability.]

By using this abstraction, functions can obey
link:https://en.wikipedia.org/wiki/Robustness_principle[Postel's law], and take advantage of the
link:https://en.wikipedia.org/wiki/Liskov_substitution_principle[Liskov substitution principle], such that
the function's code does not need to change depending on where the data is coming from (local file or
remote URI), and such that the caller of the function has maximum control over where it sources the data,
as well as what is received (it can choose to download the data if necessary, or to not download the data
and pass it by reference to some later consumer).

`Source` objects should generally be created via `thds.core.source.from_file` or
`thds.core.source.from_uri`, though if you have an `AdlsFqn` or `AdlsHashedResource` you may use
`thds.adls.source.from_adls` directly to avoid needing to convert the `AdlsFqn` into a string.

Functions consuming read-only data should define their parameter types as `Source`, and functions
returning read-only data should use `Source` in the return type as well. The function code may operate
using `Path` or whatever else is convenient when creating new files, but prior to return, any file being
returned should be converted into a `Source`, usually via `from_file`.

- `source.from_file` will automagically assign a remote URI name for your returned Source(s), based on
  the invocation output URI plus the base name of the file (e.g.
  `/var/tmp/t4/9__2bhc914s58rs3gdf3d4t00000gr/snow_white.txt` ->
  `adls://ds/tmp/foobar/pipeline/thds.my_function/snow_white.txt--VaultAskBrain.hPUok14h4T8BYy1oGgoaJu9LxNV6yVM154sFaP8/snow_white.txt`).
  This is standard practice and recommended, to avoid name collisions on a shared remote blob store.
- To skip the automagic remote naming behavior, you may instead use `mops.pure.create_source_at_uri`.
  Note that this risks name collisions, in particular the possibility of overwriting data that was meant
  to be read-only for the benefit of a downstream process. You have been warned...

An example:

```python
from pathlib import Path

from thds.core.source import Source, from_file
from thds.mops import tempdir

def remote_func(src_a: Source, src_b: Source) -> Source:
    outf = Path.cwd() / 'c.txt'
    with open(outf, 'w') as wf:
        # you can `open` a src object directly
        wf.write(open(src_a).read() + '\n' + open(src_b).read())
    return from_file(outf)


def orchestrator():
    src_a = from_file('a.txt')
    src_b = from_file(Path('b.txt'))  # works with Path, too
    src_c = remote_func(src_a, src_b)
    assert open(src_c).read() == open(src_a).read() + '\n' + src_b.path().read_text()
    # sources can be opened; they can also be turned into Path objects.
```

The above code will work whether or not `mops` is in play at all, and it will produce exactly the same
results in either case, with the exception that if your `remote_func` actually runs on a different
machine, there will not be a `c.txt` in your current working directory when the function returns (since
side-effects are not propagated by `mops`). In fact, if you use `mops` but still execute locally (e.g.
with `memoize_in`), `c.txt` _will_ exist in your current working directory, and only a single upload of
the result data will take place (and no downloads of any kind), as all input data is available locally,
and the output data is already present locally after return.

NOTE: the `source.uri` for a file that **does** exist locally will be a file URI (`file://...`), whereas
the `source.uri` for a file that **does not** exist locally will be the remote URI (ex `adls://...`).

NOTE: Again, `Source` is now the standard recommendation for how to pass file-like read-only data into your
functions, and return it as a value to consumers after creation. As long as your data can be treated as
read-only after the time of initial write, you can safely stop reading this document and use `Source`
everywhere.

### pathlib.Path (deprecated)

See link:./paths.adoc[paths] for documentation on this still-supported but less well optimized
functionality.

## Non-file large shared objects

**(Upload a large object once per pipeline)**

For large non-file objects being passed many times as read-only parameters to functions, you may reduce
serialization and upload resource usage by identifying those objects as 'shared'.

Under the hood, this defers serialization until the time of upload based on the object's ID. Objects that
are 'shared' must be both pickleable and weak referenceable; noisy errors will occur if they are not.
Pandas DataFrames and Numpy NDArrays will work out of the box. Python ``list``s will not, though
subclassing can wrap them to make them weak-referenceable. Consult the Python
link:https://docs.python.org/3.8/library/weakref.html[`weakref` docs] for more details.

This is a feature of the `MemoizingPicklingRunner`, and usage will look something like this:

[source,python]
----
runner = MemoizingPicklingRunner(...)

def your_orchestrator(...):
    ...
	runner.shared(training_x_df, y=training_y_ndarr)

    ...
	the_remote_df_func(training_x_df, training_y_ndarr, ...)


@use_runner(runner)
def the_remote_df_func(x_df, y_ndarr, ...):
    ...
----

If passed as a keyword parameter, the name serves only for debugging purposes - otherwise it is
meaningless and there is no risk of collision.
