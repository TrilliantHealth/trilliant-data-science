"""Utilities built around pickle for the purpose of transferring large amounts of on-disk
data and also functions."""

import inspect
import io
import pickle
import typing as ty

# so we can pickle and re-raise exceptions with remote tracebacks
from tblib import pickling_support  # type: ignore

from thds.core import hashing, log, source

from ..core.source import prepare_source_argument, prepare_source_result
from ..core.types import Args, Kwargs, SerializerHandler, T
from ..core.uris import get_bytes
from .pickles import (
    PicklableFunction,
    UnpickleSourceHashrefArgument,
    UnpickleSourceResult,
    UnpickleSourceUriArgument,
)

logger = log.getLogger(__name__)


def wrap_f(f):
    if hasattr(f, "__module__") and hasattr(f, "__name__"):
        return PicklableFunction(f)
    return f


class _CallbackPickler(pickle.Pickler):
    def __init__(self, handlers: ty.Sequence[SerializerHandler], *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.handlers = handlers

    def persistent_id(self, obj):
        if isinstance(obj, Exception):
            pickling_support.install(obj)
        for handler in self.handlers:
            pid = handler(obj)
            if pid is not None:
                return pid
        return None


class CallableUnpickler(pickle.Unpickler):
    """Present same interface as pickle.load but support unpickling callable PIDs generated by CallbackPickler."""

    def persistent_load(self, pid):
        try:
            return pid()
        except TypeError as te:
            # logger.exception("TypeError hit while debugging")
            # this line should never get hit as long as nobody asks us to unpickle PIDs we don't know about
            raise pickle.UnpicklingError(f"unsupported persistent object - {te}")  # pragma: no cover


class Dumper:
    """Presents the same interface as pickle.dump but supports
    arbitrary callback-based unpickling.
    """

    def __init__(self, *handlers: SerializerHandler):
        self.handlers = handlers

    def __call__(self, obj: object, file: ty.IO, *args, **kwargs):
        _CallbackPickler(self.handlers, file, *args, **kwargs).dump(obj)


def gimme_bytes(pickle_dump: ty.Callable[[object, ty.IO], None], obj: object) -> bytes:
    with io.BytesIO() as bio:
        pickle_dump(obj, bio)
        bio.seek(0)
        return bio.read()


def read_partial_pickle(full_bytes: bytes) -> ty.Tuple[bytes, ty.Any]:
    # in order to be forward-compatible with v3 of mops, we're introducing a new
    # wrinkle in the read. Instead of assuming that the data at the URI
    # _begins_ with a pickle, we are looking for the first possible pickle
    # and beginning our read there. Mops 3 will be generating some human-readable,
    # non-pickle metadata and embedding it at the beginning of the file.
    first_pickle_pos = full_bytes.find(b"\x80")
    if first_pickle_pos == -1:
        raise ValueError("Unable to find a pickle in the bytes")
    return (
        full_bytes[:first_pickle_pos],
        CallableUnpickler(io.BytesIO(full_bytes[first_pickle_pos:])).load(),
    )


def make_read_object(
    type_hint: str, wrapper: ty.Callable[[ty.Any], T] = lambda o: o
) -> ty.Callable[[str], T]:
    def read_object(uri: str) -> T:
        _unused, unpickled = read_partial_pickle(get_bytes(uri, type_hint=type_hint))
        return wrapper(unpickled)

    return read_object


def freeze_args_kwargs(dumper: Dumper, f, args: Args, kwargs: Kwargs) -> bytes:
    """Returns a pickled (args, kwargs) tuple, with pre-bound
    arguments to normalize different call structures into a
    canonical/determinstic binding.

    Also binds default arguments, for maximum determinism/explicitness.
    """
    bound_arguments = inspect.signature(f).bind(*args, **kwargs)
    bound_arguments.apply_defaults()
    return gimme_bytes(dumper, (bound_arguments.args, bound_arguments.kwargs))


def unfreeze_args_kwargs(args_kwargs_pickle: bytes) -> ty.Tuple[Args, Kwargs]:
    """Undoes a freeze_args_kwargs call."""
    return CallableUnpickler(io.BytesIO(args_kwargs_pickle)).load()


# SerializerHandlers for Source objects:
_DeserSource = ty.Callable[[], source.Source]


class SourceArgumentPickler:
    """Only for use on the orchestrator side, when serializing the arguments."""

    def __call__(self, maybe_source: ty.Any) -> ty.Optional[_DeserSource]:
        if isinstance(maybe_source, source.Source):
            uri_or_hash = prepare_source_argument(maybe_source)
            if isinstance(uri_or_hash, hashing.Hash):
                return ty.cast(_DeserSource, UnpickleSourceHashrefArgument(uri_or_hash))
            return ty.cast(_DeserSource, UnpickleSourceUriArgument(uri_or_hash))
            # I do not understand why these casts are necessary to avoid mypy errors.
            # I think it has something to do with NamedTuples being the underlying
            # object type that is expected to support __call__(self) -> Foo,
            # but I haven't recently located a relevant Issue anywhere.
        return None


class SourceResultPickler:
    """Only for use on the remote side, when serializing the result."""

    def __call__(self, maybe_source: ty.Any) -> ty.Optional[_DeserSource]:
        if isinstance(maybe_source, source.Source):
            return ty.cast(_DeserSource, UnpickleSourceResult(*prepare_source_result(maybe_source)))

        return None
